{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install torch\n!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:41:27.335928Z","iopub.execute_input":"2023-07-23T10:41:27.336813Z","iopub.status.idle":"2023-07-23T10:42:03.984581Z","shell.execute_reply.started":"2023-07-23T10:41:27.336772Z","shell.execute_reply":"2023-07-23T10:42:03.983374Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers\n  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting huggingface-hub<1.0,>=0.14.1\n  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting regex!=2019.12.17\n  Downloading regex-2023.6.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.2)\nCollecting safetensors>=0.3.1\n  Downloading safetensors-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\nCollecting fsspec\n  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\nInstalling collected packages: tokenizers, safetensors, regex, fsspec, huggingface-hub, transformers\nSuccessfully installed fsspec-2023.6.0 huggingface-hub-0.16.4 regex-2023.6.3 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nRequirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (2.0.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.8/site-packages (from torch) (3.1)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.99)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.99)\nRequirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/site-packages (from torch) (2.0.0)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/site-packages (from torch) (10.9.0.58)\nRequirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/site-packages (from torch) (11.10.3.66)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from torch) (3.1.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.8/site-packages (from torch) (1.12)\nRequirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/site-packages (from torch) (8.5.0.96)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.4.91)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.101)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from torch) (3.12.2)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/site-packages (from torch) (11.4.0.1)\nRequirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/site-packages (from torch) (11.7.91)\nRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/site-packages (from torch) (2.14.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch) (4.7.1)\nRequirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/site-packages (from torch) (10.2.10.91)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (57.5.0)\nRequirement already satisfied: wheel in /usr/local/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.40.0)\nRequirement already satisfied: lit in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch) (16.0.6)\nRequirement already satisfied: cmake in /usr/local/lib/python3.8/site-packages (from triton==2.0.0->torch) (3.26.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nCollecting datasets\n  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/site-packages (from datasets) (2.0.3)\nCollecting pyarrow>=8.0.0\n  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: packaging in /usr/local/lib/python3.8/site-packages (from datasets) (23.1)\nCollecting xxhash\n  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.8/site-packages (from datasets) (0.16.4)\nCollecting multiprocess\n  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/site-packages (from datasets) (2.31.0)\nCollecting aiohttp\n  Downloading aiohttp-3.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from datasets) (6.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (3.2.0)\nCollecting aiosignal>=1.1.2\n  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting multidict<7.0,>=4.5\n  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\nCollecting frozenlist>=1.1.1\n  Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting yarl<2.0,>=1.0\n  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nCollecting multiprocess\n  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nInstalling collected packages: xxhash, pyarrow, multiprocess, multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, datasets\nSuccessfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.13.1 frozenlist-1.4.0 multidict-6.0.4 multiprocess-0.70.14 pyarrow-12.0.1 xxhash-3.2.0 yarl-1.9.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\n\n# Step 2: Prepare the dataset\n# Assuming your dataset has 'text' and 'label' columns in a CSV file\ndf = pd.read_csv('/kaggle/input/arabichatespeech/hatespeech.csv')\n\n# Step 3: Tokenize the data\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.texts = dataframe['sentence']\n        self.labels = dataframe['type']\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        text = str(self.texts[index])\n        label = int(self.labels[index])\n\n        inputs = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Step 4: Create DataLoader\ndataset = CustomDataset(df, tokenizer, max_length=128)\ntrain_dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n\n# Step 5: Define the model and training loop\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    for batch in train_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        print(f\"Epoch {epoch+1}/{epochs}, Batch Loss: {loss.item():.4f}\")\n        \nmodel.save_pretrained('modelWeights')","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:42:07.393925Z","iopub.execute_input":"2023-07-23T10:42:07.394463Z","iopub.status.idle":"2023-07-23T10:59:15.321338Z","shell.execute_reply.started":"2023-07-23T10:42:07.394382Z","shell.execute_reply":"2023-07-23T10:59:15.319810Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nDownloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 5.14MB/s]\nDownloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 5.55kB/s]\nDownloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 531kB/s]\nDownloading model.safetensors: 100%|██████████| 440M/440M [00:02<00:00, 212MB/s]  \nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3, Batch Loss: 0.6979\nEpoch 1/3, Batch Loss: 0.5960\nEpoch 1/3, Batch Loss: 0.6277\nEpoch 1/3, Batch Loss: 0.5196\nEpoch 1/3, Batch Loss: 0.6539\nEpoch 1/3, Batch Loss: 0.4139\nEpoch 1/3, Batch Loss: 0.6203\nEpoch 1/3, Batch Loss: 0.7025\nEpoch 1/3, Batch Loss: 0.4105\nEpoch 1/3, Batch Loss: 0.5961\nEpoch 1/3, Batch Loss: 0.3548\nEpoch 1/3, Batch Loss: 0.4725\nEpoch 1/3, Batch Loss: 0.5161\nEpoch 1/3, Batch Loss: 0.4804\nEpoch 1/3, Batch Loss: 0.2737\nEpoch 1/3, Batch Loss: 0.3013\nEpoch 1/3, Batch Loss: 0.5024\nEpoch 1/3, Batch Loss: 0.7031\nEpoch 1/3, Batch Loss: 0.2694\nEpoch 1/3, Batch Loss: 0.4839\nEpoch 1/3, Batch Loss: 0.3585\nEpoch 1/3, Batch Loss: 0.6107\nEpoch 1/3, Batch Loss: 0.3429\nEpoch 1/3, Batch Loss: 0.2123\nEpoch 1/3, Batch Loss: 0.7340\nEpoch 1/3, Batch Loss: 0.5540\nEpoch 1/3, Batch Loss: 0.7447\nEpoch 1/3, Batch Loss: 0.8450\nEpoch 1/3, Batch Loss: 0.3993\nEpoch 1/3, Batch Loss: 0.5741\nEpoch 1/3, Batch Loss: 0.5286\nEpoch 1/3, Batch Loss: 0.4700\nEpoch 1/3, Batch Loss: 0.6511\nEpoch 1/3, Batch Loss: 0.4027\nEpoch 1/3, Batch Loss: 0.4310\nEpoch 1/3, Batch Loss: 0.4325\nEpoch 1/3, Batch Loss: 0.4248\nEpoch 1/3, Batch Loss: 0.4555\nEpoch 1/3, Batch Loss: 0.4848\nEpoch 1/3, Batch Loss: 0.4762\nEpoch 1/3, Batch Loss: 0.3352\nEpoch 1/3, Batch Loss: 0.4533\nEpoch 1/3, Batch Loss: 0.4778\nEpoch 1/3, Batch Loss: 0.5419\nEpoch 1/3, Batch Loss: 0.6944\nEpoch 1/3, Batch Loss: 0.5603\nEpoch 1/3, Batch Loss: 0.6207\nEpoch 1/3, Batch Loss: 0.4620\nEpoch 1/3, Batch Loss: 0.2882\nEpoch 1/3, Batch Loss: 0.5427\nEpoch 1/3, Batch Loss: 0.4925\nEpoch 1/3, Batch Loss: 0.6143\nEpoch 1/3, Batch Loss: 0.7267\nEpoch 1/3, Batch Loss: 0.5011\nEpoch 1/3, Batch Loss: 0.2885\nEpoch 1/3, Batch Loss: 0.4251\nEpoch 1/3, Batch Loss: 0.4032\nEpoch 1/3, Batch Loss: 0.6383\nEpoch 1/3, Batch Loss: 0.3762\nEpoch 1/3, Batch Loss: 0.6247\nEpoch 1/3, Batch Loss: 0.4372\nEpoch 1/3, Batch Loss: 0.3790\nEpoch 1/3, Batch Loss: 0.5596\nEpoch 1/3, Batch Loss: 0.4006\nEpoch 1/3, Batch Loss: 0.5987\nEpoch 1/3, Batch Loss: 0.2951\nEpoch 1/3, Batch Loss: 0.3796\nEpoch 1/3, Batch Loss: 0.4574\nEpoch 1/3, Batch Loss: 0.3751\nEpoch 1/3, Batch Loss: 0.4853\nEpoch 1/3, Batch Loss: 0.4807\nEpoch 1/3, Batch Loss: 0.4225\nEpoch 1/3, Batch Loss: 0.7198\nEpoch 1/3, Batch Loss: 0.5253\nEpoch 1/3, Batch Loss: 0.6401\nEpoch 1/3, Batch Loss: 0.3015\nEpoch 1/3, Batch Loss: 0.5571\nEpoch 1/3, Batch Loss: 0.4572\nEpoch 1/3, Batch Loss: 0.6802\nEpoch 1/3, Batch Loss: 0.6115\nEpoch 1/3, Batch Loss: 0.6939\nEpoch 1/3, Batch Loss: 0.4329\nEpoch 1/3, Batch Loss: 0.4389\nEpoch 1/3, Batch Loss: 0.4781\nEpoch 1/3, Batch Loss: 0.8149\nEpoch 1/3, Batch Loss: 0.4502\nEpoch 1/3, Batch Loss: 0.4442\nEpoch 1/3, Batch Loss: 0.3823\nEpoch 1/3, Batch Loss: 0.5292\nEpoch 1/3, Batch Loss: 0.4981\nEpoch 1/3, Batch Loss: 0.5477\nEpoch 1/3, Batch Loss: 0.4420\nEpoch 1/3, Batch Loss: 0.6710\nEpoch 1/3, Batch Loss: 0.3308\nEpoch 1/3, Batch Loss: 0.7051\nEpoch 1/3, Batch Loss: 0.6620\nEpoch 1/3, Batch Loss: 0.3690\nEpoch 1/3, Batch Loss: 0.2901\nEpoch 1/3, Batch Loss: 0.3942\nEpoch 1/3, Batch Loss: 0.6059\nEpoch 1/3, Batch Loss: 0.5291\nEpoch 1/3, Batch Loss: 0.5282\nEpoch 1/3, Batch Loss: 0.3541\nEpoch 1/3, Batch Loss: 0.6943\nEpoch 1/3, Batch Loss: 0.2941\nEpoch 1/3, Batch Loss: 0.4662\nEpoch 1/3, Batch Loss: 0.3622\nEpoch 1/3, Batch Loss: 0.8436\nEpoch 1/3, Batch Loss: 0.6194\nEpoch 1/3, Batch Loss: 0.4959\nEpoch 1/3, Batch Loss: 0.5204\nEpoch 1/3, Batch Loss: 0.4774\nEpoch 1/3, Batch Loss: 0.5836\nEpoch 1/3, Batch Loss: 0.6991\nEpoch 1/3, Batch Loss: 0.4661\nEpoch 1/3, Batch Loss: 0.5775\nEpoch 1/3, Batch Loss: 0.6236\nEpoch 1/3, Batch Loss: 0.5279\nEpoch 1/3, Batch Loss: 0.5172\nEpoch 1/3, Batch Loss: 0.4779\nEpoch 1/3, Batch Loss: 0.3838\nEpoch 1/3, Batch Loss: 0.3897\nEpoch 1/3, Batch Loss: 0.3788\nEpoch 1/3, Batch Loss: 0.7039\nEpoch 1/3, Batch Loss: 0.3225\nEpoch 1/3, Batch Loss: 0.4810\nEpoch 1/3, Batch Loss: 0.4560\nEpoch 1/3, Batch Loss: 0.5531\nEpoch 1/3, Batch Loss: 0.7114\nEpoch 1/3, Batch Loss: 0.4906\nEpoch 1/3, Batch Loss: 0.4774\nEpoch 1/3, Batch Loss: 0.3895\nEpoch 1/3, Batch Loss: 0.4638\nEpoch 1/3, Batch Loss: 0.5774\nEpoch 1/3, Batch Loss: 0.5101\nEpoch 1/3, Batch Loss: 0.3804\nEpoch 1/3, Batch Loss: 0.3630\nEpoch 1/3, Batch Loss: 0.3062\nEpoch 1/3, Batch Loss: 0.6338\nEpoch 1/3, Batch Loss: 0.2804\nEpoch 1/3, Batch Loss: 0.8121\nEpoch 1/3, Batch Loss: 0.3086\nEpoch 1/3, Batch Loss: 0.5309\nEpoch 1/3, Batch Loss: 0.2367\nEpoch 1/3, Batch Loss: 0.3492\nEpoch 1/3, Batch Loss: 0.4519\nEpoch 1/3, Batch Loss: 0.4354\nEpoch 1/3, Batch Loss: 0.8350\nEpoch 1/3, Batch Loss: 0.6348\nEpoch 1/3, Batch Loss: 0.4979\nEpoch 1/3, Batch Loss: 0.2106\nEpoch 1/3, Batch Loss: 0.4731\nEpoch 1/3, Batch Loss: 0.6139\nEpoch 1/3, Batch Loss: 0.5939\nEpoch 1/3, Batch Loss: 0.6003\nEpoch 1/3, Batch Loss: 0.5354\nEpoch 1/3, Batch Loss: 0.5949\nEpoch 1/3, Batch Loss: 0.3759\nEpoch 1/3, Batch Loss: 0.5296\nEpoch 1/3, Batch Loss: 0.4267\nEpoch 1/3, Batch Loss: 0.3951\nEpoch 1/3, Batch Loss: 0.3582\nEpoch 1/3, Batch Loss: 0.6042\nEpoch 1/3, Batch Loss: 0.3100\nEpoch 1/3, Batch Loss: 0.4204\nEpoch 1/3, Batch Loss: 0.2471\nEpoch 1/3, Batch Loss: 0.2842\nEpoch 1/3, Batch Loss: 0.3861\nEpoch 1/3, Batch Loss: 0.6797\nEpoch 1/3, Batch Loss: 0.4293\nEpoch 1/3, Batch Loss: 0.4882\nEpoch 1/3, Batch Loss: 0.7266\nEpoch 1/3, Batch Loss: 0.4327\nEpoch 1/3, Batch Loss: 0.6586\nEpoch 1/3, Batch Loss: 0.4466\nEpoch 1/3, Batch Loss: 0.2872\nEpoch 1/3, Batch Loss: 0.2471\nEpoch 1/3, Batch Loss: 0.6471\nEpoch 1/3, Batch Loss: 0.5143\nEpoch 1/3, Batch Loss: 0.2834\nEpoch 1/3, Batch Loss: 0.5895\nEpoch 1/3, Batch Loss: 0.5739\nEpoch 1/3, Batch Loss: 0.3724\nEpoch 1/3, Batch Loss: 0.4962\nEpoch 1/3, Batch Loss: 0.3948\nEpoch 1/3, Batch Loss: 0.5679\nEpoch 1/3, Batch Loss: 0.5029\nEpoch 1/3, Batch Loss: 0.4741\nEpoch 1/3, Batch Loss: 0.5025\nEpoch 1/3, Batch Loss: 0.3291\nEpoch 1/3, Batch Loss: 0.2259\nEpoch 1/3, Batch Loss: 0.4442\nEpoch 1/3, Batch Loss: 0.2956\nEpoch 1/3, Batch Loss: 0.4477\nEpoch 1/3, Batch Loss: 0.4986\nEpoch 1/3, Batch Loss: 0.7654\nEpoch 1/3, Batch Loss: 0.3184\nEpoch 1/3, Batch Loss: 0.5245\nEpoch 1/3, Batch Loss: 0.8361\nEpoch 1/3, Batch Loss: 0.7023\nEpoch 1/3, Batch Loss: 0.3529\nEpoch 1/3, Batch Loss: 0.3515\nEpoch 1/3, Batch Loss: 0.6716\nEpoch 1/3, Batch Loss: 0.4937\nEpoch 1/3, Batch Loss: 0.3197\nEpoch 1/3, Batch Loss: 0.2959\nEpoch 1/3, Batch Loss: 0.3061\nEpoch 1/3, Batch Loss: 0.3285\nEpoch 1/3, Batch Loss: 0.4687\nEpoch 1/3, Batch Loss: 0.4859\nEpoch 1/3, Batch Loss: 0.5922\nEpoch 1/3, Batch Loss: 0.6598\nEpoch 1/3, Batch Loss: 0.4722\nEpoch 1/3, Batch Loss: 0.4079\nEpoch 1/3, Batch Loss: 0.4479\nEpoch 1/3, Batch Loss: 0.6517\nEpoch 1/3, Batch Loss: 0.3842\nEpoch 1/3, Batch Loss: 0.2639\nEpoch 1/3, Batch Loss: 0.2597\nEpoch 1/3, Batch Loss: 0.2371\nEpoch 1/3, Batch Loss: 0.3423\nEpoch 1/3, Batch Loss: 0.3822\nEpoch 1/3, Batch Loss: 0.5928\nEpoch 1/3, Batch Loss: 0.4543\nEpoch 1/3, Batch Loss: 0.4271\nEpoch 1/3, Batch Loss: 0.3319\nEpoch 1/3, Batch Loss: 0.3779\nEpoch 1/3, Batch Loss: 0.2914\nEpoch 1/3, Batch Loss: 0.5108\nEpoch 1/3, Batch Loss: 0.3602\nEpoch 1/3, Batch Loss: 0.5436\nEpoch 1/3, Batch Loss: 0.4169\nEpoch 1/3, Batch Loss: 0.2917\nEpoch 1/3, Batch Loss: 0.3668\nEpoch 1/3, Batch Loss: 0.6637\nEpoch 1/3, Batch Loss: 0.4644\nEpoch 1/3, Batch Loss: 0.3293\nEpoch 1/3, Batch Loss: 0.4778\nEpoch 1/3, Batch Loss: 0.5798\nEpoch 1/3, Batch Loss: 0.2876\nEpoch 1/3, Batch Loss: 0.7825\nEpoch 1/3, Batch Loss: 0.4203\nEpoch 1/3, Batch Loss: 0.4231\nEpoch 1/3, Batch Loss: 0.3619\nEpoch 1/3, Batch Loss: 0.2456\nEpoch 1/3, Batch Loss: 0.5083\nEpoch 1/3, Batch Loss: 0.4044\nEpoch 1/3, Batch Loss: 0.4522\nEpoch 1/3, Batch Loss: 0.4477\nEpoch 1/3, Batch Loss: 0.3464\nEpoch 1/3, Batch Loss: 0.3413\nEpoch 1/3, Batch Loss: 0.5667\nEpoch 1/3, Batch Loss: 0.8628\nEpoch 1/3, Batch Loss: 0.5712\nEpoch 1/3, Batch Loss: 0.2577\nEpoch 1/3, Batch Loss: 0.5500\nEpoch 1/3, Batch Loss: 0.1629\nEpoch 1/3, Batch Loss: 0.2496\nEpoch 1/3, Batch Loss: 0.2640\nEpoch 1/3, Batch Loss: 0.7161\nEpoch 1/3, Batch Loss: 0.2712\nEpoch 1/3, Batch Loss: 0.2827\nEpoch 1/3, Batch Loss: 0.0998\nEpoch 1/3, Batch Loss: 0.4654\nEpoch 1/3, Batch Loss: 0.4780\nEpoch 1/3, Batch Loss: 0.4573\nEpoch 1/3, Batch Loss: 0.6924\nEpoch 1/3, Batch Loss: 0.4886\nEpoch 1/3, Batch Loss: 0.6012\nEpoch 1/3, Batch Loss: 0.5316\nEpoch 1/3, Batch Loss: 0.3740\nEpoch 1/3, Batch Loss: 0.6045\nEpoch 1/3, Batch Loss: 0.5010\nEpoch 1/3, Batch Loss: 0.3870\nEpoch 1/3, Batch Loss: 0.4751\nEpoch 1/3, Batch Loss: 0.3680\nEpoch 1/3, Batch Loss: 0.3402\nEpoch 1/3, Batch Loss: 0.4840\nEpoch 1/3, Batch Loss: 0.3005\nEpoch 1/3, Batch Loss: 0.3201\nEpoch 1/3, Batch Loss: 0.3334\nEpoch 1/3, Batch Loss: 0.4956\nEpoch 1/3, Batch Loss: 0.3710\nEpoch 1/3, Batch Loss: 0.4467\nEpoch 1/3, Batch Loss: 0.2060\nEpoch 1/3, Batch Loss: 0.5100\nEpoch 1/3, Batch Loss: 0.5698\nEpoch 1/3, Batch Loss: 0.6366\nEpoch 1/3, Batch Loss: 0.6167\nEpoch 1/3, Batch Loss: 0.5257\nEpoch 1/3, Batch Loss: 0.5554\nEpoch 1/3, Batch Loss: 0.4384\nEpoch 1/3, Batch Loss: 0.3050\nEpoch 1/3, Batch Loss: 0.3296\nEpoch 1/3, Batch Loss: 0.5660\nEpoch 1/3, Batch Loss: 0.3836\nEpoch 1/3, Batch Loss: 0.5066\nEpoch 1/3, Batch Loss: 0.5459\nEpoch 1/3, Batch Loss: 0.6941\nEpoch 1/3, Batch Loss: 0.2345\nEpoch 1/3, Batch Loss: 0.4239\nEpoch 1/3, Batch Loss: 0.4435\nEpoch 1/3, Batch Loss: 0.5555\nEpoch 1/3, Batch Loss: 0.7187\nEpoch 1/3, Batch Loss: 0.2860\nEpoch 1/3, Batch Loss: 0.2925\nEpoch 1/3, Batch Loss: 0.2544\nEpoch 1/3, Batch Loss: 0.3433\nEpoch 1/3, Batch Loss: 0.5391\nEpoch 1/3, Batch Loss: 0.5328\nEpoch 1/3, Batch Loss: 0.5269\nEpoch 1/3, Batch Loss: 0.6137\nEpoch 1/3, Batch Loss: 0.4844\nEpoch 1/3, Batch Loss: 0.1281\nEpoch 1/3, Batch Loss: 0.5817\nEpoch 1/3, Batch Loss: 0.3249\nEpoch 1/3, Batch Loss: 0.6136\nEpoch 1/3, Batch Loss: 0.5687\nEpoch 1/3, Batch Loss: 0.4273\nEpoch 1/3, Batch Loss: 0.2438\nEpoch 1/3, Batch Loss: 0.3354\nEpoch 1/3, Batch Loss: 0.3400\nEpoch 1/3, Batch Loss: 0.4762\nEpoch 1/3, Batch Loss: 0.2886\nEpoch 1/3, Batch Loss: 0.9656\nEpoch 1/3, Batch Loss: 0.4649\nEpoch 1/3, Batch Loss: 0.4616\nEpoch 1/3, Batch Loss: 0.3633\nEpoch 1/3, Batch Loss: 0.4514\nEpoch 1/3, Batch Loss: 0.3461\nEpoch 1/3, Batch Loss: 0.5095\nEpoch 1/3, Batch Loss: 0.4293\nEpoch 1/3, Batch Loss: 0.3612\nEpoch 1/3, Batch Loss: 0.4629\nEpoch 1/3, Batch Loss: 0.4624\nEpoch 1/3, Batch Loss: 0.2999\nEpoch 1/3, Batch Loss: 0.4622\nEpoch 1/3, Batch Loss: 0.4831\nEpoch 1/3, Batch Loss: 0.5372\nEpoch 1/3, Batch Loss: 0.3543\nEpoch 1/3, Batch Loss: 0.4377\nEpoch 1/3, Batch Loss: 0.2332\nEpoch 1/3, Batch Loss: 0.4337\nEpoch 1/3, Batch Loss: 0.5494\nEpoch 1/3, Batch Loss: 0.4695\nEpoch 1/3, Batch Loss: 0.3711\nEpoch 1/3, Batch Loss: 0.1855\nEpoch 1/3, Batch Loss: 0.4123\nEpoch 1/3, Batch Loss: 0.3355\nEpoch 1/3, Batch Loss: 0.4094\nEpoch 1/3, Batch Loss: 0.3855\nEpoch 1/3, Batch Loss: 0.5499\nEpoch 1/3, Batch Loss: 0.4007\nEpoch 1/3, Batch Loss: 0.3659\nEpoch 1/3, Batch Loss: 0.1939\nEpoch 1/3, Batch Loss: 0.4511\nEpoch 1/3, Batch Loss: 0.6505\nEpoch 1/3, Batch Loss: 0.4147\nEpoch 1/3, Batch Loss: 0.3787\nEpoch 1/3, Batch Loss: 0.5168\nEpoch 1/3, Batch Loss: 0.2647\nEpoch 1/3, Batch Loss: 0.5088\nEpoch 1/3, Batch Loss: 0.4069\nEpoch 1/3, Batch Loss: 0.4163\nEpoch 1/3, Batch Loss: 0.4277\nEpoch 1/3, Batch Loss: 0.2507\nEpoch 1/3, Batch Loss: 0.3140\nEpoch 1/3, Batch Loss: 0.3632\nEpoch 1/3, Batch Loss: 0.5527\nEpoch 1/3, Batch Loss: 0.1921\nEpoch 1/3, Batch Loss: 0.4180\nEpoch 1/3, Batch Loss: 0.1657\nEpoch 1/3, Batch Loss: 0.0806\nEpoch 1/3, Batch Loss: 0.3070\nEpoch 1/3, Batch Loss: 0.2544\nEpoch 1/3, Batch Loss: 0.5186\nEpoch 1/3, Batch Loss: 0.7481\nEpoch 1/3, Batch Loss: 0.5602\nEpoch 1/3, Batch Loss: 0.2670\nEpoch 1/3, Batch Loss: 0.7680\nEpoch 1/3, Batch Loss: 0.3346\nEpoch 1/3, Batch Loss: 0.4813\nEpoch 1/3, Batch Loss: 0.4572\nEpoch 1/3, Batch Loss: 0.4681\nEpoch 1/3, Batch Loss: 0.4755\nEpoch 1/3, Batch Loss: 0.4881\nEpoch 1/3, Batch Loss: 0.4223\nEpoch 1/3, Batch Loss: 0.4434\nEpoch 1/3, Batch Loss: 0.2661\nEpoch 1/3, Batch Loss: 0.6129\nEpoch 1/3, Batch Loss: 0.3857\nEpoch 1/3, Batch Loss: 0.3853\nEpoch 1/3, Batch Loss: 0.3267\nEpoch 1/3, Batch Loss: 0.5979\nEpoch 1/3, Batch Loss: 0.3030\nEpoch 1/3, Batch Loss: 0.2618\nEpoch 1/3, Batch Loss: 0.2827\nEpoch 1/3, Batch Loss: 0.4902\nEpoch 1/3, Batch Loss: 0.2189\nEpoch 1/3, Batch Loss: 0.3943\nEpoch 1/3, Batch Loss: 0.3857\nEpoch 1/3, Batch Loss: 0.3331\nEpoch 1/3, Batch Loss: 0.3760\nEpoch 1/3, Batch Loss: 0.1727\nEpoch 1/3, Batch Loss: 0.4862\nEpoch 1/3, Batch Loss: 0.3498\nEpoch 1/3, Batch Loss: 0.2085\nEpoch 1/3, Batch Loss: 0.5060\nEpoch 1/3, Batch Loss: 0.5104\nEpoch 1/3, Batch Loss: 0.3004\nEpoch 1/3, Batch Loss: 0.2627\nEpoch 1/3, Batch Loss: 0.3989\nEpoch 1/3, Batch Loss: 0.4560\nEpoch 1/3, Batch Loss: 0.4123\nEpoch 1/3, Batch Loss: 0.4582\nEpoch 1/3, Batch Loss: 0.4832\nEpoch 1/3, Batch Loss: 0.4292\nEpoch 1/3, Batch Loss: 0.6036\nEpoch 1/3, Batch Loss: 0.5802\nEpoch 1/3, Batch Loss: 0.1963\nEpoch 1/3, Batch Loss: 0.2138\nEpoch 1/3, Batch Loss: 0.4536\nEpoch 1/3, Batch Loss: 0.2924\nEpoch 1/3, Batch Loss: 0.4292\nEpoch 1/3, Batch Loss: 0.4411\nEpoch 1/3, Batch Loss: 0.4128\nEpoch 1/3, Batch Loss: 0.4686\nEpoch 1/3, Batch Loss: 0.2304\nEpoch 1/3, Batch Loss: 0.4455\nEpoch 1/3, Batch Loss: 0.5697\nEpoch 1/3, Batch Loss: 0.4448\nEpoch 1/3, Batch Loss: 0.7312\nEpoch 1/3, Batch Loss: 0.4857\nEpoch 1/3, Batch Loss: 0.3494\nEpoch 1/3, Batch Loss: 0.3615\nEpoch 1/3, Batch Loss: 0.3322\nEpoch 1/3, Batch Loss: 0.2284\nEpoch 1/3, Batch Loss: 0.3368\nEpoch 2/3, Batch Loss: 0.1631\nEpoch 2/3, Batch Loss: 0.1450\nEpoch 2/3, Batch Loss: 1.1252\nEpoch 2/3, Batch Loss: 0.3246\nEpoch 2/3, Batch Loss: 0.5551\nEpoch 2/3, Batch Loss: 0.4017\nEpoch 2/3, Batch Loss: 0.2981\nEpoch 2/3, Batch Loss: 0.3119\nEpoch 2/3, Batch Loss: 0.2833\nEpoch 2/3, Batch Loss: 0.4967\nEpoch 2/3, Batch Loss: 0.2600\nEpoch 2/3, Batch Loss: 0.4251\nEpoch 2/3, Batch Loss: 0.3942\nEpoch 2/3, Batch Loss: 0.5329\nEpoch 2/3, Batch Loss: 0.3203\nEpoch 2/3, Batch Loss: 0.3835\nEpoch 2/3, Batch Loss: 0.2393\nEpoch 2/3, Batch Loss: 0.5179\nEpoch 2/3, Batch Loss: 0.4315\nEpoch 2/3, Batch Loss: 0.3426\nEpoch 2/3, Batch Loss: 0.3411\nEpoch 2/3, Batch Loss: 0.2647\nEpoch 2/3, Batch Loss: 0.3027\nEpoch 2/3, Batch Loss: 0.4267\nEpoch 2/3, Batch Loss: 0.4759\nEpoch 2/3, Batch Loss: 0.3623\nEpoch 2/3, Batch Loss: 0.1316\nEpoch 2/3, Batch Loss: 0.5147\nEpoch 2/3, Batch Loss: 0.5790\nEpoch 2/3, Batch Loss: 0.5730\nEpoch 2/3, Batch Loss: 0.3599\nEpoch 2/3, Batch Loss: 0.2314\nEpoch 2/3, Batch Loss: 0.2439\nEpoch 2/3, Batch Loss: 0.3840\nEpoch 2/3, Batch Loss: 0.5086\nEpoch 2/3, Batch Loss: 0.3916\nEpoch 2/3, Batch Loss: 0.2860\nEpoch 2/3, Batch Loss: 0.3552\nEpoch 2/3, Batch Loss: 0.2313\nEpoch 2/3, Batch Loss: 0.2403\nEpoch 2/3, Batch Loss: 0.0807\nEpoch 2/3, Batch Loss: 0.2854\nEpoch 2/3, Batch Loss: 0.6750\nEpoch 2/3, Batch Loss: 0.6555\nEpoch 2/3, Batch Loss: 0.3219\nEpoch 2/3, Batch Loss: 0.4560\nEpoch 2/3, Batch Loss: 0.5196\nEpoch 2/3, Batch Loss: 0.3635\nEpoch 2/3, Batch Loss: 0.3333\nEpoch 2/3, Batch Loss: 0.3417\nEpoch 2/3, Batch Loss: 0.5273\nEpoch 2/3, Batch Loss: 0.5613\nEpoch 2/3, Batch Loss: 0.4294\nEpoch 2/3, Batch Loss: 0.5198\nEpoch 2/3, Batch Loss: 0.5086\nEpoch 2/3, Batch Loss: 0.4579\nEpoch 2/3, Batch Loss: 0.4302\nEpoch 2/3, Batch Loss: 0.4283\nEpoch 2/3, Batch Loss: 0.4455\nEpoch 2/3, Batch Loss: 0.4430\nEpoch 2/3, Batch Loss: 0.2643\nEpoch 2/3, Batch Loss: 0.3948\nEpoch 2/3, Batch Loss: 0.4134\nEpoch 2/3, Batch Loss: 0.3874\nEpoch 2/3, Batch Loss: 0.4885\nEpoch 2/3, Batch Loss: 0.4369\nEpoch 2/3, Batch Loss: 0.3010\nEpoch 2/3, Batch Loss: 0.2458\nEpoch 2/3, Batch Loss: 0.2863\nEpoch 2/3, Batch Loss: 0.2525\nEpoch 2/3, Batch Loss: 0.1972\nEpoch 2/3, Batch Loss: 0.4228\nEpoch 2/3, Batch Loss: 0.2024\nEpoch 2/3, Batch Loss: 0.4575\nEpoch 2/3, Batch Loss: 0.3100\nEpoch 2/3, Batch Loss: 0.2618\nEpoch 2/3, Batch Loss: 0.6479\nEpoch 2/3, Batch Loss: 0.4451\nEpoch 2/3, Batch Loss: 0.4989\nEpoch 2/3, Batch Loss: 0.3670\nEpoch 2/3, Batch Loss: 0.2086\nEpoch 2/3, Batch Loss: 0.4408\nEpoch 2/3, Batch Loss: 0.7055\nEpoch 2/3, Batch Loss: 0.2345\nEpoch 2/3, Batch Loss: 0.1755\nEpoch 2/3, Batch Loss: 0.3254\nEpoch 2/3, Batch Loss: 0.3213\nEpoch 2/3, Batch Loss: 0.4292\nEpoch 2/3, Batch Loss: 0.2236\nEpoch 2/3, Batch Loss: 0.2476\nEpoch 2/3, Batch Loss: 0.3599\nEpoch 2/3, Batch Loss: 0.2802\nEpoch 2/3, Batch Loss: 0.3477\nEpoch 2/3, Batch Loss: 0.1359\nEpoch 2/3, Batch Loss: 0.2587\nEpoch 2/3, Batch Loss: 0.2221\nEpoch 2/3, Batch Loss: 0.5091\nEpoch 2/3, Batch Loss: 0.2507\nEpoch 2/3, Batch Loss: 0.2661\nEpoch 2/3, Batch Loss: 0.6396\nEpoch 2/3, Batch Loss: 0.3370\nEpoch 2/3, Batch Loss: 0.6342\nEpoch 2/3, Batch Loss: 0.2276\nEpoch 2/3, Batch Loss: 0.3835\nEpoch 2/3, Batch Loss: 0.3977\nEpoch 2/3, Batch Loss: 0.2489\nEpoch 2/3, Batch Loss: 0.3716\nEpoch 2/3, Batch Loss: 0.3379\nEpoch 2/3, Batch Loss: 0.4389\nEpoch 2/3, Batch Loss: 0.2587\nEpoch 2/3, Batch Loss: 0.0928\nEpoch 2/3, Batch Loss: 0.2397\nEpoch 2/3, Batch Loss: 0.9975\nEpoch 2/3, Batch Loss: 0.2817\nEpoch 2/3, Batch Loss: 0.3614\nEpoch 2/3, Batch Loss: 0.5839\nEpoch 2/3, Batch Loss: 0.3673\nEpoch 2/3, Batch Loss: 0.4645\nEpoch 2/3, Batch Loss: 0.3322\nEpoch 2/3, Batch Loss: 0.4998\nEpoch 2/3, Batch Loss: 0.3150\nEpoch 2/3, Batch Loss: 0.4539\nEpoch 2/3, Batch Loss: 0.3029\nEpoch 2/3, Batch Loss: 0.3917\nEpoch 2/3, Batch Loss: 0.3857\nEpoch 2/3, Batch Loss: 0.1618\nEpoch 2/3, Batch Loss: 0.3636\nEpoch 2/3, Batch Loss: 0.2570\nEpoch 2/3, Batch Loss: 0.3993\nEpoch 2/3, Batch Loss: 0.4508\nEpoch 2/3, Batch Loss: 0.2621\nEpoch 2/3, Batch Loss: 0.5734\nEpoch 2/3, Batch Loss: 0.1057\nEpoch 2/3, Batch Loss: 0.8039\nEpoch 2/3, Batch Loss: 0.5719\nEpoch 2/3, Batch Loss: 0.3964\nEpoch 2/3, Batch Loss: 0.2931\nEpoch 2/3, Batch Loss: 0.5712\nEpoch 2/3, Batch Loss: 0.3254\nEpoch 2/3, Batch Loss: 0.5107\nEpoch 2/3, Batch Loss: 0.4030\nEpoch 2/3, Batch Loss: 0.4138\nEpoch 2/3, Batch Loss: 0.2387\nEpoch 2/3, Batch Loss: 0.3412\nEpoch 2/3, Batch Loss: 0.6690\nEpoch 2/3, Batch Loss: 0.3586\nEpoch 2/3, Batch Loss: 0.1757\nEpoch 2/3, Batch Loss: 0.2217\nEpoch 2/3, Batch Loss: 0.1889\nEpoch 2/3, Batch Loss: 0.2965\nEpoch 2/3, Batch Loss: 0.3477\nEpoch 2/3, Batch Loss: 0.4258\nEpoch 2/3, Batch Loss: 0.3615\nEpoch 2/3, Batch Loss: 0.7258\nEpoch 2/3, Batch Loss: 0.1896\nEpoch 2/3, Batch Loss: 0.4960\nEpoch 2/3, Batch Loss: 0.5400\nEpoch 2/3, Batch Loss: 0.4492\nEpoch 2/3, Batch Loss: 0.3560\nEpoch 2/3, Batch Loss: 0.5877\nEpoch 2/3, Batch Loss: 0.4907\nEpoch 2/3, Batch Loss: 0.3858\nEpoch 2/3, Batch Loss: 0.5433\nEpoch 2/3, Batch Loss: 0.7043\nEpoch 2/3, Batch Loss: 0.5710\nEpoch 2/3, Batch Loss: 0.1614\nEpoch 2/3, Batch Loss: 0.2502\nEpoch 2/3, Batch Loss: 0.6181\nEpoch 2/3, Batch Loss: 0.1876\nEpoch 2/3, Batch Loss: 0.4162\nEpoch 2/3, Batch Loss: 0.4297\nEpoch 2/3, Batch Loss: 0.3471\nEpoch 2/3, Batch Loss: 0.3336\nEpoch 2/3, Batch Loss: 0.2201\nEpoch 2/3, Batch Loss: 0.1730\nEpoch 2/3, Batch Loss: 0.2804\nEpoch 2/3, Batch Loss: 0.4266\nEpoch 2/3, Batch Loss: 0.2665\nEpoch 2/3, Batch Loss: 0.2500\nEpoch 2/3, Batch Loss: 0.2603\nEpoch 2/3, Batch Loss: 0.3140\nEpoch 2/3, Batch Loss: 1.2271\nEpoch 2/3, Batch Loss: 0.2789\nEpoch 2/3, Batch Loss: 0.5599\nEpoch 2/3, Batch Loss: 0.4719\nEpoch 2/3, Batch Loss: 0.4414\nEpoch 2/3, Batch Loss: 0.6064\nEpoch 2/3, Batch Loss: 0.4436\nEpoch 2/3, Batch Loss: 0.2774\nEpoch 2/3, Batch Loss: 0.5870\nEpoch 2/3, Batch Loss: 0.4361\nEpoch 2/3, Batch Loss: 0.3445\nEpoch 2/3, Batch Loss: 0.4110\nEpoch 2/3, Batch Loss: 0.3832\nEpoch 2/3, Batch Loss: 0.4516\nEpoch 2/3, Batch Loss: 0.2120\nEpoch 2/3, Batch Loss: 0.5207\nEpoch 2/3, Batch Loss: 0.3733\nEpoch 2/3, Batch Loss: 0.2498\nEpoch 2/3, Batch Loss: 0.6433\nEpoch 2/3, Batch Loss: 0.2605\nEpoch 2/3, Batch Loss: 0.3285\nEpoch 2/3, Batch Loss: 0.3627\nEpoch 2/3, Batch Loss: 0.3123\nEpoch 2/3, Batch Loss: 0.6125\nEpoch 2/3, Batch Loss: 0.2870\nEpoch 2/3, Batch Loss: 0.5056\nEpoch 2/3, Batch Loss: 0.3288\nEpoch 2/3, Batch Loss: 0.3892\nEpoch 2/3, Batch Loss: 0.2856\nEpoch 2/3, Batch Loss: 0.5743\nEpoch 2/3, Batch Loss: 0.2184\nEpoch 2/3, Batch Loss: 0.3149\nEpoch 2/3, Batch Loss: 0.5626\nEpoch 2/3, Batch Loss: 0.1519\nEpoch 2/3, Batch Loss: 0.4149\nEpoch 2/3, Batch Loss: 0.3613\nEpoch 2/3, Batch Loss: 0.2267\nEpoch 2/3, Batch Loss: 0.3312\nEpoch 2/3, Batch Loss: 0.2945\nEpoch 2/3, Batch Loss: 0.5707\nEpoch 2/3, Batch Loss: 0.3281\nEpoch 2/3, Batch Loss: 0.4386\nEpoch 2/3, Batch Loss: 0.3082\nEpoch 2/3, Batch Loss: 0.3091\nEpoch 2/3, Batch Loss: 0.2318\nEpoch 2/3, Batch Loss: 0.4696\nEpoch 2/3, Batch Loss: 0.2298\nEpoch 2/3, Batch Loss: 0.4315\nEpoch 2/3, Batch Loss: 0.3407\nEpoch 2/3, Batch Loss: 0.4049\nEpoch 2/3, Batch Loss: 0.2672\nEpoch 2/3, Batch Loss: 0.4792\nEpoch 2/3, Batch Loss: 0.3112\nEpoch 2/3, Batch Loss: 0.4404\nEpoch 2/3, Batch Loss: 0.4175\nEpoch 2/3, Batch Loss: 0.3689\nEpoch 2/3, Batch Loss: 0.3129\nEpoch 2/3, Batch Loss: 0.3072\nEpoch 2/3, Batch Loss: 0.3310\nEpoch 2/3, Batch Loss: 0.4887\nEpoch 2/3, Batch Loss: 0.5004\nEpoch 2/3, Batch Loss: 0.1777\nEpoch 2/3, Batch Loss: 0.4052\nEpoch 2/3, Batch Loss: 0.3448\nEpoch 2/3, Batch Loss: 0.3982\nEpoch 2/3, Batch Loss: 0.3284\nEpoch 2/3, Batch Loss: 0.4332\nEpoch 2/3, Batch Loss: 0.1441\nEpoch 2/3, Batch Loss: 0.2255\nEpoch 2/3, Batch Loss: 0.3778\nEpoch 2/3, Batch Loss: 0.2499\nEpoch 2/3, Batch Loss: 0.3608\nEpoch 2/3, Batch Loss: 0.3947\nEpoch 2/3, Batch Loss: 0.3934\nEpoch 2/3, Batch Loss: 0.4769\nEpoch 2/3, Batch Loss: 0.3399\nEpoch 2/3, Batch Loss: 0.3716\nEpoch 2/3, Batch Loss: 0.4197\nEpoch 2/3, Batch Loss: 0.2581\nEpoch 2/3, Batch Loss: 0.2881\nEpoch 2/3, Batch Loss: 0.3817\nEpoch 2/3, Batch Loss: 0.2312\nEpoch 2/3, Batch Loss: 0.3065\nEpoch 2/3, Batch Loss: 0.3250\nEpoch 2/3, Batch Loss: 0.2228\nEpoch 2/3, Batch Loss: 0.1246\nEpoch 2/3, Batch Loss: 0.3937\nEpoch 2/3, Batch Loss: 0.1993\nEpoch 2/3, Batch Loss: 0.2500\nEpoch 2/3, Batch Loss: 0.3686\nEpoch 2/3, Batch Loss: 0.4653\nEpoch 2/3, Batch Loss: 0.2054\nEpoch 2/3, Batch Loss: 0.3726\nEpoch 2/3, Batch Loss: 0.3199\nEpoch 2/3, Batch Loss: 0.3509\nEpoch 2/3, Batch Loss: 0.4967\nEpoch 2/3, Batch Loss: 0.6251\nEpoch 2/3, Batch Loss: 0.3222\nEpoch 2/3, Batch Loss: 0.2740\nEpoch 2/3, Batch Loss: 0.1850\nEpoch 2/3, Batch Loss: 0.4204\nEpoch 2/3, Batch Loss: 0.4157\nEpoch 2/3, Batch Loss: 0.2566\nEpoch 2/3, Batch Loss: 0.3135\nEpoch 2/3, Batch Loss: 0.3198\nEpoch 2/3, Batch Loss: 0.5693\nEpoch 2/3, Batch Loss: 0.1508\nEpoch 2/3, Batch Loss: 0.3021\nEpoch 2/3, Batch Loss: 0.2687\nEpoch 2/3, Batch Loss: 0.2013\nEpoch 2/3, Batch Loss: 0.2493\nEpoch 2/3, Batch Loss: 0.1533\nEpoch 2/3, Batch Loss: 0.4939\nEpoch 2/3, Batch Loss: 0.1921\nEpoch 2/3, Batch Loss: 0.5265\nEpoch 2/3, Batch Loss: 0.5187\nEpoch 2/3, Batch Loss: 0.1046\nEpoch 2/3, Batch Loss: 0.1576\nEpoch 2/3, Batch Loss: 0.2536\nEpoch 2/3, Batch Loss: 0.3888\nEpoch 2/3, Batch Loss: 0.2538\nEpoch 2/3, Batch Loss: 0.3662\nEpoch 2/3, Batch Loss: 0.5638\nEpoch 2/3, Batch Loss: 0.2011\nEpoch 2/3, Batch Loss: 0.0734\nEpoch 2/3, Batch Loss: 0.1223\nEpoch 2/3, Batch Loss: 0.3686\nEpoch 2/3, Batch Loss: 0.3590\nEpoch 2/3, Batch Loss: 0.2547\nEpoch 2/3, Batch Loss: 0.2760\nEpoch 2/3, Batch Loss: 0.4897\nEpoch 2/3, Batch Loss: 0.4270\nEpoch 2/3, Batch Loss: 0.2755\nEpoch 2/3, Batch Loss: 0.2146\nEpoch 2/3, Batch Loss: 0.4992\nEpoch 2/3, Batch Loss: 0.6485\nEpoch 2/3, Batch Loss: 0.4959\nEpoch 2/3, Batch Loss: 0.3410\nEpoch 2/3, Batch Loss: 0.5322\nEpoch 2/3, Batch Loss: 0.4721\nEpoch 2/3, Batch Loss: 0.2878\nEpoch 2/3, Batch Loss: 0.0926\nEpoch 2/3, Batch Loss: 0.5671\nEpoch 2/3, Batch Loss: 0.0837\nEpoch 2/3, Batch Loss: 0.1575\nEpoch 2/3, Batch Loss: 0.3582\nEpoch 2/3, Batch Loss: 0.2159\nEpoch 2/3, Batch Loss: 0.4558\nEpoch 2/3, Batch Loss: 0.3539\nEpoch 2/3, Batch Loss: 0.5103\nEpoch 2/3, Batch Loss: 0.2953\nEpoch 2/3, Batch Loss: 0.4010\nEpoch 2/3, Batch Loss: 0.2265\nEpoch 2/3, Batch Loss: 0.1372\nEpoch 2/3, Batch Loss: 0.7347\nEpoch 2/3, Batch Loss: 0.2029\nEpoch 2/3, Batch Loss: 0.4608\nEpoch 2/3, Batch Loss: 0.3589\nEpoch 2/3, Batch Loss: 0.3121\nEpoch 2/3, Batch Loss: 0.3304\nEpoch 2/3, Batch Loss: 0.2834\nEpoch 2/3, Batch Loss: 0.3466\nEpoch 2/3, Batch Loss: 0.3638\nEpoch 2/3, Batch Loss: 0.2672\nEpoch 2/3, Batch Loss: 0.2688\nEpoch 2/3, Batch Loss: 0.3564\nEpoch 2/3, Batch Loss: 0.2725\nEpoch 2/3, Batch Loss: 0.2148\nEpoch 2/3, Batch Loss: 0.2038\nEpoch 2/3, Batch Loss: 0.3023\nEpoch 2/3, Batch Loss: 0.1662\nEpoch 2/3, Batch Loss: 0.3962\nEpoch 2/3, Batch Loss: 1.1754\nEpoch 2/3, Batch Loss: 0.1413\nEpoch 2/3, Batch Loss: 0.2353\nEpoch 2/3, Batch Loss: 0.5195\nEpoch 2/3, Batch Loss: 0.3875\nEpoch 2/3, Batch Loss: 0.2545\nEpoch 2/3, Batch Loss: 0.2156\nEpoch 2/3, Batch Loss: 0.3179\nEpoch 2/3, Batch Loss: 0.2982\nEpoch 2/3, Batch Loss: 0.3555\nEpoch 2/3, Batch Loss: 0.4751\nEpoch 2/3, Batch Loss: 0.2437\nEpoch 2/3, Batch Loss: 0.3433\nEpoch 2/3, Batch Loss: 0.1679\nEpoch 2/3, Batch Loss: 0.1954\nEpoch 2/3, Batch Loss: 0.3047\nEpoch 2/3, Batch Loss: 0.0963\nEpoch 2/3, Batch Loss: 0.6513\nEpoch 2/3, Batch Loss: 0.6886\nEpoch 2/3, Batch Loss: 0.5762\nEpoch 2/3, Batch Loss: 0.3769\nEpoch 2/3, Batch Loss: 0.5512\nEpoch 2/3, Batch Loss: 0.3273\nEpoch 2/3, Batch Loss: 0.4920\nEpoch 2/3, Batch Loss: 0.4686\nEpoch 2/3, Batch Loss: 0.2574\nEpoch 2/3, Batch Loss: 0.3940\nEpoch 2/3, Batch Loss: 0.4172\nEpoch 2/3, Batch Loss: 0.4005\nEpoch 2/3, Batch Loss: 0.3775\nEpoch 2/3, Batch Loss: 0.3717\nEpoch 2/3, Batch Loss: 0.3038\nEpoch 2/3, Batch Loss: 0.4270\nEpoch 2/3, Batch Loss: 0.1980\nEpoch 2/3, Batch Loss: 0.1959\nEpoch 2/3, Batch Loss: 0.4371\nEpoch 2/3, Batch Loss: 0.4471\nEpoch 2/3, Batch Loss: 0.8317\nEpoch 2/3, Batch Loss: 0.1467\nEpoch 2/3, Batch Loss: 0.1820\nEpoch 2/3, Batch Loss: 0.4252\nEpoch 2/3, Batch Loss: 0.2289\nEpoch 2/3, Batch Loss: 0.1819\nEpoch 2/3, Batch Loss: 0.4829\nEpoch 2/3, Batch Loss: 0.4113\nEpoch 2/3, Batch Loss: 0.3798\nEpoch 2/3, Batch Loss: 0.4320\nEpoch 2/3, Batch Loss: 0.4459\nEpoch 2/3, Batch Loss: 0.3634\nEpoch 2/3, Batch Loss: 0.4470\nEpoch 2/3, Batch Loss: 0.2705\nEpoch 2/3, Batch Loss: 0.1888\nEpoch 2/3, Batch Loss: 0.2894\nEpoch 2/3, Batch Loss: 0.2192\nEpoch 2/3, Batch Loss: 0.6768\nEpoch 2/3, Batch Loss: 0.2044\nEpoch 2/3, Batch Loss: 0.4735\nEpoch 2/3, Batch Loss: 0.4209\nEpoch 2/3, Batch Loss: 0.3500\nEpoch 2/3, Batch Loss: 0.3738\nEpoch 2/3, Batch Loss: 0.5247\nEpoch 2/3, Batch Loss: 0.3187\nEpoch 2/3, Batch Loss: 0.3224\nEpoch 2/3, Batch Loss: 0.3755\nEpoch 2/3, Batch Loss: 0.3513\nEpoch 2/3, Batch Loss: 0.2481\nEpoch 2/3, Batch Loss: 0.5361\nEpoch 2/3, Batch Loss: 0.4857\nEpoch 2/3, Batch Loss: 0.2653\nEpoch 2/3, Batch Loss: 0.3206\nEpoch 2/3, Batch Loss: 0.1685\nEpoch 2/3, Batch Loss: 0.3666\nEpoch 2/3, Batch Loss: 0.2513\nEpoch 2/3, Batch Loss: 0.3121\nEpoch 2/3, Batch Loss: 0.3704\nEpoch 2/3, Batch Loss: 0.3197\nEpoch 2/3, Batch Loss: 0.4896\nEpoch 2/3, Batch Loss: 0.3167\nEpoch 2/3, Batch Loss: 0.4459\nEpoch 2/3, Batch Loss: 0.1351\nEpoch 2/3, Batch Loss: 0.4035\nEpoch 2/3, Batch Loss: 0.7046\nEpoch 2/3, Batch Loss: 0.6051\nEpoch 2/3, Batch Loss: 0.3483\nEpoch 2/3, Batch Loss: 0.7672\nEpoch 3/3, Batch Loss: 0.3999\nEpoch 3/3, Batch Loss: 0.2470\nEpoch 3/3, Batch Loss: 0.3106\nEpoch 3/3, Batch Loss: 0.2756\nEpoch 3/3, Batch Loss: 0.2867\nEpoch 3/3, Batch Loss: 0.4444\nEpoch 3/3, Batch Loss: 0.5728\nEpoch 3/3, Batch Loss: 0.3587\nEpoch 3/3, Batch Loss: 0.1866\nEpoch 3/3, Batch Loss: 0.1370\nEpoch 3/3, Batch Loss: 0.1615\nEpoch 3/3, Batch Loss: 0.2785\nEpoch 3/3, Batch Loss: 0.3186\nEpoch 3/3, Batch Loss: 0.2381\nEpoch 3/3, Batch Loss: 0.2287\nEpoch 3/3, Batch Loss: 0.4347\nEpoch 3/3, Batch Loss: 0.3064\nEpoch 3/3, Batch Loss: 0.2495\nEpoch 3/3, Batch Loss: 0.2686\nEpoch 3/3, Batch Loss: 0.4237\nEpoch 3/3, Batch Loss: 0.1731\nEpoch 3/3, Batch Loss: 0.3150\nEpoch 3/3, Batch Loss: 0.2176\nEpoch 3/3, Batch Loss: 0.3390\nEpoch 3/3, Batch Loss: 0.2712\nEpoch 3/3, Batch Loss: 0.2478\nEpoch 3/3, Batch Loss: 0.2850\nEpoch 3/3, Batch Loss: 0.2593\nEpoch 3/3, Batch Loss: 0.2861\nEpoch 3/3, Batch Loss: 0.2367\nEpoch 3/3, Batch Loss: 0.2391\nEpoch 3/3, Batch Loss: 0.2471\nEpoch 3/3, Batch Loss: 0.3967\nEpoch 3/3, Batch Loss: 0.2693\nEpoch 3/3, Batch Loss: 0.5464\nEpoch 3/3, Batch Loss: 0.1445\nEpoch 3/3, Batch Loss: 0.1364\nEpoch 3/3, Batch Loss: 0.1632\nEpoch 3/3, Batch Loss: 0.4001\nEpoch 3/3, Batch Loss: 0.3883\nEpoch 3/3, Batch Loss: 0.2010\nEpoch 3/3, Batch Loss: 0.2154\nEpoch 3/3, Batch Loss: 0.2141\nEpoch 3/3, Batch Loss: 0.4749\nEpoch 3/3, Batch Loss: 0.1540\nEpoch 3/3, Batch Loss: 0.1789\nEpoch 3/3, Batch Loss: 0.6371\nEpoch 3/3, Batch Loss: 0.1097\nEpoch 3/3, Batch Loss: 0.4831\nEpoch 3/3, Batch Loss: 0.3520\nEpoch 3/3, Batch Loss: 0.1775\nEpoch 3/3, Batch Loss: 0.2722\nEpoch 3/3, Batch Loss: 0.2942\nEpoch 3/3, Batch Loss: 0.3187\nEpoch 3/3, Batch Loss: 0.3006\nEpoch 3/3, Batch Loss: 0.2322\nEpoch 3/3, Batch Loss: 0.2645\nEpoch 3/3, Batch Loss: 0.3570\nEpoch 3/3, Batch Loss: 0.3551\nEpoch 3/3, Batch Loss: 0.5488\nEpoch 3/3, Batch Loss: 0.4305\nEpoch 3/3, Batch Loss: 0.4300\nEpoch 3/3, Batch Loss: 0.5017\nEpoch 3/3, Batch Loss: 0.4523\nEpoch 3/3, Batch Loss: 0.1263\nEpoch 3/3, Batch Loss: 0.2850\nEpoch 3/3, Batch Loss: 0.2728\nEpoch 3/3, Batch Loss: 0.3455\nEpoch 3/3, Batch Loss: 0.3578\nEpoch 3/3, Batch Loss: 0.2979\nEpoch 3/3, Batch Loss: 0.2882\nEpoch 3/3, Batch Loss: 0.4234\nEpoch 3/3, Batch Loss: 0.4158\nEpoch 3/3, Batch Loss: 0.2718\nEpoch 3/3, Batch Loss: 0.3966\nEpoch 3/3, Batch Loss: 0.1351\nEpoch 3/3, Batch Loss: 0.2346\nEpoch 3/3, Batch Loss: 0.2915\nEpoch 3/3, Batch Loss: 0.4175\nEpoch 3/3, Batch Loss: 0.1326\nEpoch 3/3, Batch Loss: 0.1241\nEpoch 3/3, Batch Loss: 0.1417\nEpoch 3/3, Batch Loss: 0.1932\nEpoch 3/3, Batch Loss: 0.3550\nEpoch 3/3, Batch Loss: 0.0961\nEpoch 3/3, Batch Loss: 0.3404\nEpoch 3/3, Batch Loss: 0.2535\nEpoch 3/3, Batch Loss: 0.1820\nEpoch 3/3, Batch Loss: 0.3839\nEpoch 3/3, Batch Loss: 0.1677\nEpoch 3/3, Batch Loss: 0.3472\nEpoch 3/3, Batch Loss: 0.2844\nEpoch 3/3, Batch Loss: 0.3188\nEpoch 3/3, Batch Loss: 0.2087\nEpoch 3/3, Batch Loss: 0.3253\nEpoch 3/3, Batch Loss: 0.2722\nEpoch 3/3, Batch Loss: 0.2341\nEpoch 3/3, Batch Loss: 0.1468\nEpoch 3/3, Batch Loss: 0.1106\nEpoch 3/3, Batch Loss: 0.2997\nEpoch 3/3, Batch Loss: 0.0496\nEpoch 3/3, Batch Loss: 0.3150\nEpoch 3/3, Batch Loss: 0.0406\nEpoch 3/3, Batch Loss: 0.3308\nEpoch 3/3, Batch Loss: 0.1432\nEpoch 3/3, Batch Loss: 0.3218\nEpoch 3/3, Batch Loss: 0.1108\nEpoch 3/3, Batch Loss: 0.2395\nEpoch 3/3, Batch Loss: 0.4519\nEpoch 3/3, Batch Loss: 0.1985\nEpoch 3/3, Batch Loss: 0.4602\nEpoch 3/3, Batch Loss: 0.2992\nEpoch 3/3, Batch Loss: 0.2909\nEpoch 3/3, Batch Loss: 0.4322\nEpoch 3/3, Batch Loss: 0.3445\nEpoch 3/3, Batch Loss: 0.4366\nEpoch 3/3, Batch Loss: 0.3444\nEpoch 3/3, Batch Loss: 0.1919\nEpoch 3/3, Batch Loss: 0.3977\nEpoch 3/3, Batch Loss: 0.2900\nEpoch 3/3, Batch Loss: 0.3896\nEpoch 3/3, Batch Loss: 0.3387\nEpoch 3/3, Batch Loss: 0.1636\nEpoch 3/3, Batch Loss: 0.4139\nEpoch 3/3, Batch Loss: 0.2602\nEpoch 3/3, Batch Loss: 0.2067\nEpoch 3/3, Batch Loss: 0.2201\nEpoch 3/3, Batch Loss: 0.2129\nEpoch 3/3, Batch Loss: 0.2963\nEpoch 3/3, Batch Loss: 0.3373\nEpoch 3/3, Batch Loss: 0.1287\nEpoch 3/3, Batch Loss: 0.4718\nEpoch 3/3, Batch Loss: 0.1396\nEpoch 3/3, Batch Loss: 0.3582\nEpoch 3/3, Batch Loss: 0.3080\nEpoch 3/3, Batch Loss: 0.1921\nEpoch 3/3, Batch Loss: 0.2057\nEpoch 3/3, Batch Loss: 0.2284\nEpoch 3/3, Batch Loss: 0.2673\nEpoch 3/3, Batch Loss: 0.1646\nEpoch 3/3, Batch Loss: 0.3132\nEpoch 3/3, Batch Loss: 0.3064\nEpoch 3/3, Batch Loss: 0.1689\nEpoch 3/3, Batch Loss: 0.1665\nEpoch 3/3, Batch Loss: 0.1692\nEpoch 3/3, Batch Loss: 0.4417\nEpoch 3/3, Batch Loss: 0.1048\nEpoch 3/3, Batch Loss: 0.1686\nEpoch 3/3, Batch Loss: 0.2651\nEpoch 3/3, Batch Loss: 0.5033\nEpoch 3/3, Batch Loss: 0.3894\nEpoch 3/3, Batch Loss: 0.1478\nEpoch 3/3, Batch Loss: 0.1831\nEpoch 3/3, Batch Loss: 0.4279\nEpoch 3/3, Batch Loss: 0.5298\nEpoch 3/3, Batch Loss: 0.0948\nEpoch 3/3, Batch Loss: 0.3802\nEpoch 3/3, Batch Loss: 0.5015\nEpoch 3/3, Batch Loss: 0.2152\nEpoch 3/3, Batch Loss: 0.1708\nEpoch 3/3, Batch Loss: 0.3695\nEpoch 3/3, Batch Loss: 0.2024\nEpoch 3/3, Batch Loss: 0.1517\nEpoch 3/3, Batch Loss: 0.1714\nEpoch 3/3, Batch Loss: 0.2176\nEpoch 3/3, Batch Loss: 0.4198\nEpoch 3/3, Batch Loss: 0.4985\nEpoch 3/3, Batch Loss: 0.0772\nEpoch 3/3, Batch Loss: 0.2013\nEpoch 3/3, Batch Loss: 0.2452\nEpoch 3/3, Batch Loss: 0.7762\nEpoch 3/3, Batch Loss: 0.5319\nEpoch 3/3, Batch Loss: 0.1217\nEpoch 3/3, Batch Loss: 0.1950\nEpoch 3/3, Batch Loss: 0.2262\nEpoch 3/3, Batch Loss: 0.2392\nEpoch 3/3, Batch Loss: 0.3484\nEpoch 3/3, Batch Loss: 0.6422\nEpoch 3/3, Batch Loss: 0.4466\nEpoch 3/3, Batch Loss: 0.3104\nEpoch 3/3, Batch Loss: 0.3071\nEpoch 3/3, Batch Loss: 0.1169\nEpoch 3/3, Batch Loss: 0.5237\nEpoch 3/3, Batch Loss: 0.3472\nEpoch 3/3, Batch Loss: 0.2662\nEpoch 3/3, Batch Loss: 0.2270\nEpoch 3/3, Batch Loss: 0.1720\nEpoch 3/3, Batch Loss: 0.4829\nEpoch 3/3, Batch Loss: 0.1799\nEpoch 3/3, Batch Loss: 0.3772\nEpoch 3/3, Batch Loss: 0.3084\nEpoch 3/3, Batch Loss: 0.2836\nEpoch 3/3, Batch Loss: 0.0955\nEpoch 3/3, Batch Loss: 0.3814\nEpoch 3/3, Batch Loss: 0.5603\nEpoch 3/3, Batch Loss: 0.2926\nEpoch 3/3, Batch Loss: 0.2719\nEpoch 3/3, Batch Loss: 0.2196\nEpoch 3/3, Batch Loss: 0.2442\nEpoch 3/3, Batch Loss: 0.2553\nEpoch 3/3, Batch Loss: 0.1067\nEpoch 3/3, Batch Loss: 0.4612\nEpoch 3/3, Batch Loss: 0.2601\nEpoch 3/3, Batch Loss: 0.1726\nEpoch 3/3, Batch Loss: 0.2028\nEpoch 3/3, Batch Loss: 0.3033\nEpoch 3/3, Batch Loss: 0.6225\nEpoch 3/3, Batch Loss: 0.0750\nEpoch 3/3, Batch Loss: 0.1316\nEpoch 3/3, Batch Loss: 0.3042\nEpoch 3/3, Batch Loss: 0.1230\nEpoch 3/3, Batch Loss: 0.0552\nEpoch 3/3, Batch Loss: 0.3395\nEpoch 3/3, Batch Loss: 0.3017\nEpoch 3/3, Batch Loss: 0.1649\nEpoch 3/3, Batch Loss: 0.2325\nEpoch 3/3, Batch Loss: 0.3118\nEpoch 3/3, Batch Loss: 0.2853\nEpoch 3/3, Batch Loss: 0.3144\nEpoch 3/3, Batch Loss: 0.3661\nEpoch 3/3, Batch Loss: 0.5141\nEpoch 3/3, Batch Loss: 0.2117\nEpoch 3/3, Batch Loss: 0.4118\nEpoch 3/3, Batch Loss: 0.4799\nEpoch 3/3, Batch Loss: 0.2867\nEpoch 3/3, Batch Loss: 0.4902\nEpoch 3/3, Batch Loss: 0.5250\nEpoch 3/3, Batch Loss: 0.2134\nEpoch 3/3, Batch Loss: 0.7743\nEpoch 3/3, Batch Loss: 0.2105\nEpoch 3/3, Batch Loss: 0.5739\nEpoch 3/3, Batch Loss: 0.2940\nEpoch 3/3, Batch Loss: 0.2533\nEpoch 3/3, Batch Loss: 0.1456\nEpoch 3/3, Batch Loss: 0.3405\nEpoch 3/3, Batch Loss: 0.6756\nEpoch 3/3, Batch Loss: 0.3457\nEpoch 3/3, Batch Loss: 0.5508\nEpoch 3/3, Batch Loss: 0.3334\nEpoch 3/3, Batch Loss: 0.2878\nEpoch 3/3, Batch Loss: 0.2728\nEpoch 3/3, Batch Loss: 0.2550\nEpoch 3/3, Batch Loss: 0.3301\nEpoch 3/3, Batch Loss: 0.2531\nEpoch 3/3, Batch Loss: 0.2093\nEpoch 3/3, Batch Loss: 0.1720\nEpoch 3/3, Batch Loss: 0.2813\nEpoch 3/3, Batch Loss: 0.2702\nEpoch 3/3, Batch Loss: 0.2226\nEpoch 3/3, Batch Loss: 0.1321\nEpoch 3/3, Batch Loss: 0.6004\nEpoch 3/3, Batch Loss: 0.2398\nEpoch 3/3, Batch Loss: 0.1782\nEpoch 3/3, Batch Loss: 0.3626\nEpoch 3/3, Batch Loss: 0.0528\nEpoch 3/3, Batch Loss: 0.3627\nEpoch 3/3, Batch Loss: 0.0918\nEpoch 3/3, Batch Loss: 0.0875\nEpoch 3/3, Batch Loss: 0.1935\nEpoch 3/3, Batch Loss: 0.0747\nEpoch 3/3, Batch Loss: 0.1191\nEpoch 3/3, Batch Loss: 0.2201\nEpoch 3/3, Batch Loss: 0.2874\nEpoch 3/3, Batch Loss: 0.1820\nEpoch 3/3, Batch Loss: 0.3697\nEpoch 3/3, Batch Loss: 0.5838\nEpoch 3/3, Batch Loss: 0.2448\nEpoch 3/3, Batch Loss: 0.1921\nEpoch 3/3, Batch Loss: 0.3125\nEpoch 3/3, Batch Loss: 0.4599\nEpoch 3/3, Batch Loss: 0.2018\nEpoch 3/3, Batch Loss: 0.4152\nEpoch 3/3, Batch Loss: 0.3450\nEpoch 3/3, Batch Loss: 0.1423\nEpoch 3/3, Batch Loss: 0.3347\nEpoch 3/3, Batch Loss: 0.6001\nEpoch 3/3, Batch Loss: 0.1572\nEpoch 3/3, Batch Loss: 0.1871\nEpoch 3/3, Batch Loss: 0.1611\nEpoch 3/3, Batch Loss: 0.3822\nEpoch 3/3, Batch Loss: 0.3941\nEpoch 3/3, Batch Loss: 0.1525\nEpoch 3/3, Batch Loss: 0.1508\nEpoch 3/3, Batch Loss: 0.1870\nEpoch 3/3, Batch Loss: 0.4104\nEpoch 3/3, Batch Loss: 0.2993\nEpoch 3/3, Batch Loss: 0.1037\nEpoch 3/3, Batch Loss: 0.4458\nEpoch 3/3, Batch Loss: 0.2513\nEpoch 3/3, Batch Loss: 0.2212\nEpoch 3/3, Batch Loss: 0.2261\nEpoch 3/3, Batch Loss: 0.2825\nEpoch 3/3, Batch Loss: 0.3114\nEpoch 3/3, Batch Loss: 0.3661\nEpoch 3/3, Batch Loss: 0.5450\nEpoch 3/3, Batch Loss: 0.3931\nEpoch 3/3, Batch Loss: 0.2910\nEpoch 3/3, Batch Loss: 0.1958\nEpoch 3/3, Batch Loss: 0.3084\nEpoch 3/3, Batch Loss: 0.2465\nEpoch 3/3, Batch Loss: 0.3977\nEpoch 3/3, Batch Loss: 0.2538\nEpoch 3/3, Batch Loss: 0.1993\nEpoch 3/3, Batch Loss: 0.1089\nEpoch 3/3, Batch Loss: 0.2447\nEpoch 3/3, Batch Loss: 0.2221\nEpoch 3/3, Batch Loss: 0.3991\nEpoch 3/3, Batch Loss: 0.2099\nEpoch 3/3, Batch Loss: 0.2157\nEpoch 3/3, Batch Loss: 0.0978\nEpoch 3/3, Batch Loss: 0.7367\nEpoch 3/3, Batch Loss: 0.2443\nEpoch 3/3, Batch Loss: 0.3542\nEpoch 3/3, Batch Loss: 0.0980\nEpoch 3/3, Batch Loss: 0.1408\nEpoch 3/3, Batch Loss: 0.1574\nEpoch 3/3, Batch Loss: 0.2747\nEpoch 3/3, Batch Loss: 0.2605\nEpoch 3/3, Batch Loss: 0.1151\nEpoch 3/3, Batch Loss: 0.1189\nEpoch 3/3, Batch Loss: 0.3466\nEpoch 3/3, Batch Loss: 0.2679\nEpoch 3/3, Batch Loss: 0.0461\nEpoch 3/3, Batch Loss: 0.1181\nEpoch 3/3, Batch Loss: 0.5203\nEpoch 3/3, Batch Loss: 0.3009\nEpoch 3/3, Batch Loss: 0.4272\nEpoch 3/3, Batch Loss: 0.1649\nEpoch 3/3, Batch Loss: 0.2618\nEpoch 3/3, Batch Loss: 0.3085\nEpoch 3/3, Batch Loss: 0.2548\nEpoch 3/3, Batch Loss: 0.3153\nEpoch 3/3, Batch Loss: 0.2419\nEpoch 3/3, Batch Loss: 0.1871\nEpoch 3/3, Batch Loss: 0.1858\nEpoch 3/3, Batch Loss: 0.2192\nEpoch 3/3, Batch Loss: 0.2277\nEpoch 3/3, Batch Loss: 0.1888\nEpoch 3/3, Batch Loss: 0.1627\nEpoch 3/3, Batch Loss: 0.1938\nEpoch 3/3, Batch Loss: 0.3081\nEpoch 3/3, Batch Loss: 0.3981\nEpoch 3/3, Batch Loss: 0.1846\nEpoch 3/3, Batch Loss: 0.3078\nEpoch 3/3, Batch Loss: 0.1861\nEpoch 3/3, Batch Loss: 0.2268\nEpoch 3/3, Batch Loss: 0.3532\nEpoch 3/3, Batch Loss: 0.0697\nEpoch 3/3, Batch Loss: 0.1444\nEpoch 3/3, Batch Loss: 0.2338\nEpoch 3/3, Batch Loss: 0.4987\nEpoch 3/3, Batch Loss: 0.2398\nEpoch 3/3, Batch Loss: 0.4111\nEpoch 3/3, Batch Loss: 0.4065\nEpoch 3/3, Batch Loss: 0.2522\nEpoch 3/3, Batch Loss: 0.1272\nEpoch 3/3, Batch Loss: 0.6302\nEpoch 3/3, Batch Loss: 0.1806\nEpoch 3/3, Batch Loss: 0.1815\nEpoch 3/3, Batch Loss: 0.3408\nEpoch 3/3, Batch Loss: 0.2061\nEpoch 3/3, Batch Loss: 0.2633\nEpoch 3/3, Batch Loss: 0.5590\nEpoch 3/3, Batch Loss: 0.4196\nEpoch 3/3, Batch Loss: 0.3742\nEpoch 3/3, Batch Loss: 0.3876\nEpoch 3/3, Batch Loss: 0.2731\nEpoch 3/3, Batch Loss: 0.3490\nEpoch 3/3, Batch Loss: 0.3552\nEpoch 3/3, Batch Loss: 0.2445\nEpoch 3/3, Batch Loss: 0.1400\nEpoch 3/3, Batch Loss: 0.2995\nEpoch 3/3, Batch Loss: 0.1769\nEpoch 3/3, Batch Loss: 0.4983\nEpoch 3/3, Batch Loss: 0.3097\nEpoch 3/3, Batch Loss: 0.6482\nEpoch 3/3, Batch Loss: 0.2739\nEpoch 3/3, Batch Loss: 0.3191\nEpoch 3/3, Batch Loss: 0.1472\nEpoch 3/3, Batch Loss: 0.2050\nEpoch 3/3, Batch Loss: 0.3153\nEpoch 3/3, Batch Loss: 0.2704\nEpoch 3/3, Batch Loss: 0.1420\nEpoch 3/3, Batch Loss: 0.3519\nEpoch 3/3, Batch Loss: 0.3085\nEpoch 3/3, Batch Loss: 0.2993\nEpoch 3/3, Batch Loss: 0.0825\nEpoch 3/3, Batch Loss: 0.1288\nEpoch 3/3, Batch Loss: 0.2036\nEpoch 3/3, Batch Loss: 0.2897\nEpoch 3/3, Batch Loss: 0.1147\nEpoch 3/3, Batch Loss: 0.2065\nEpoch 3/3, Batch Loss: 0.0859\nEpoch 3/3, Batch Loss: 0.2446\nEpoch 3/3, Batch Loss: 0.4881\nEpoch 3/3, Batch Loss: 0.6005\nEpoch 3/3, Batch Loss: 0.3468\nEpoch 3/3, Batch Loss: 0.1907\nEpoch 3/3, Batch Loss: 0.3361\nEpoch 3/3, Batch Loss: 0.3079\nEpoch 3/3, Batch Loss: 0.1273\nEpoch 3/3, Batch Loss: 0.3232\nEpoch 3/3, Batch Loss: 0.2017\nEpoch 3/3, Batch Loss: 0.3896\nEpoch 3/3, Batch Loss: 0.3835\nEpoch 3/3, Batch Loss: 0.4622\nEpoch 3/3, Batch Loss: 0.0922\nEpoch 3/3, Batch Loss: 0.3640\nEpoch 3/3, Batch Loss: 0.1281\nEpoch 3/3, Batch Loss: 0.3573\nEpoch 3/3, Batch Loss: 0.0661\nEpoch 3/3, Batch Loss: 0.0517\nEpoch 3/3, Batch Loss: 0.9232\nEpoch 3/3, Batch Loss: 0.5001\nEpoch 3/3, Batch Loss: 0.3314\nEpoch 3/3, Batch Loss: 0.1061\nEpoch 3/3, Batch Loss: 0.2118\nEpoch 3/3, Batch Loss: 0.2230\nEpoch 3/3, Batch Loss: 0.4207\nEpoch 3/3, Batch Loss: 0.3575\nEpoch 3/3, Batch Loss: 0.2200\nEpoch 3/3, Batch Loss: 0.2448\nEpoch 3/3, Batch Loss: 0.2849\nEpoch 3/3, Batch Loss: 0.2965\nEpoch 3/3, Batch Loss: 0.2448\nEpoch 3/3, Batch Loss: 0.3218\nEpoch 3/3, Batch Loss: 0.2957\nEpoch 3/3, Batch Loss: 0.1555\nEpoch 3/3, Batch Loss: 0.2050\nEpoch 3/3, Batch Loss: 0.4279\nEpoch 3/3, Batch Loss: 0.3046\nEpoch 3/3, Batch Loss: 0.5339\nEpoch 3/3, Batch Loss: 0.1463\nEpoch 3/3, Batch Loss: 0.2954\nEpoch 3/3, Batch Loss: 0.1521\nEpoch 3/3, Batch Loss: 0.4667\nEpoch 3/3, Batch Loss: 0.3585\nEpoch 3/3, Batch Loss: 0.2622\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the trained model and tokenizer\nmodel_path = 'path_to_save_model'  # Replace with the path to your saved model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained(model_path)\n\n# Function to classify new text\ndef classify_text(text):\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n        input_ids = inputs['input_ids'].to(device)\n        attention_mask = inputs['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probabilities = torch.softmax(logits, dim=1)\n        predicted_label = torch.argmax(probabilities, dim=1).item()\n\n        # Assuming your model is binary classification (0 or 1)\n        if predicted_label == 1:\n            return \"Hate Speech\"\n        else:\n            return \"Not Hate Speech\"","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:59:15.323756Z","iopub.execute_input":"2023-07-23T10:59:15.324122Z","iopub.status.idle":"2023-07-23T10:59:18.690858Z","shell.execute_reply.started":"2023-07-23T10:59:15.324091Z","shell.execute_reply":"2023-07-23T10:59:18.689127Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 261\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/path_to_save_model/resolve/main/config.json","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/huggingface_hub/file_download.py:1541\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1532\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[0;32m-> 1541\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m     )\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-64bd0803-15b5a5b77b6cc4f1394c0310;ee8a582f-94ca-4c79-ac6c-e550aadbdd64)\n\nRepository Not Found for url: https://huggingface.co/path_to_save_model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_save_model\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the path to your saved model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Function to classify new text\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_text\u001b[39m(text):\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/modeling_utils.py:2325\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2324\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m config \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2325\u001b[0m     config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2335\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2336\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_auto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_auto_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2337\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2338\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2339\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2341\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py:590\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m revision\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 590\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type:\n\u001b[1;32m    592\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    593\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a model of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to instantiate a model of type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    619\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    686\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n","\u001b[0;31mOSError\u001b[0m: path_to_save_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."],"ename":"OSError","evalue":"path_to_save_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.","output_type":"error"}]},{"cell_type":"code","source":"\nnew_text = \"لا\"\npredicted_label = classify_text(new_text)\nprint(\"Predicted Label:\", predicted_label)","metadata":{"execution":{"iopub.status.busy":"2023-07-23T10:59:18.692825Z","iopub.status.idle":"2023-07-23T10:59:18.693233Z","shell.execute_reply.started":"2023-07-23T10:59:18.693036Z","shell.execute_reply":"2023-07-23T10:59:18.693055Z"},"trusted":true},"execution_count":null,"outputs":[]}]}